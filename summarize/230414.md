# base SAM classifier
The most wonderful progress of the SAM classifier is its ability to detect and segment objects from images without needing to specify the object beforehand. This feature is crucial for detecting rare types of objects that serve as landmarks or triggers in classification tasks.
## method
In Retinopathy of Prematurity (ROP) diagnosis, ophthalmologists typically detect white lines at the end of blood vessels. Instead of visualizing the exit model to know if it foucuss on the white line, we propose a method that utilizes the SAM classifier. This method involves the following steps:
1. Annotate key points of white lines in some fundus images.
2. Train an object detection model to obtain the coordinates of the white lines.
3. Sample surrounding points to reduce errors.
4. Use the coordinates to segment the white line from the background.
5. Enhance the data by removing the spatial features of the white line.
6. Classify the segmentation mask for each sampled point.
## Advantages  
The main advantages of this method are:
1. It relies on existing large-scale models to improve feature extraction from human-designed objects, resulting in better interpretability.
2. The method ignores the background and focuses purely on the object features.
3. The sampling step is expandable and parallelizable, making it suitable for real-world applications.
## Limitations
However, there is a limitation to this approach. The white lines in the early stages of ROP are extremely subtle, making their detection difficult. Furthermore, the SAM classifier may not recognize white lines in fundus images, even with coordinate guidance, because the fundus image data might not be included in their training set, let alone the white lines. As a result, this method may be more suitable for objects that are more easily recognized. In such cases, the SAM classifier can be used to ignore the background and obtain the mask for the landmarks or triggers.

## Work
### 1. Deploy the SAM locally and download the pretrained model ( 2.4G ). 
Huge thanks to the SAM community, True hero.
### 2. Review the data provided by ophthalmologists
Find two question:
#### 1. Data spilt error related to the fundus images capture
Currently, the data is split randomly. However, fundus images from the same individual tend to be similar, as people often take multiple pictures with the same gestures and camera angles. When such similar images are split into training and validation datasets, confidence in the results is reduced.

solution: 
1. Ensure that data from the same individual is not split into different datasets (in progress).
2. Abandon the data with strong similarity

#### 2. Confusion of the annotation
In some early stages of ROP, it is difficult to recognize the white lines.

solution:
1. Communicate with the ophthalmologists for clarification (in progress).

#### 3. Image condition shifts due to different environments
Fundus images taken in different environments can vary significantly. Some have a yellow background, possibly taken under incandescent lighting, while others have a black background, taken in dimmer conditions. Additionally, a small number of images are overexposed, resulting in strong light spots.

# Optic Disk Detection

1. Add annotation for given dataset 50 image
2. Stongthen the Model structure
3. Include more public avaiable datasets

# model result
Q1
data_size: train:3451, val:1150, test:1152
original Test: acc:0.815104 auc:0.907520 with 27epochs
replace_channel Test: acc:0.783854 auc:0.870236
vessel Test: acc:0.672743 auc:0.747296

Q4
the dataset has the classes: 5
data_size: train:5025, val:1675, test:1675
0 : 6247
2 : 1104
1 : 629
6 : 251
3 : 144
Test: acc:0.840597 auc:0.868945 # originial
Test: acc:0.807761 auc:0.842375 # replace blue
Test: acc:0.771940 auc:0.719125 # vessel

